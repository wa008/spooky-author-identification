{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding:utf-8\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "from sklearn.externals import joblib\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from glob import glob\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "import shutil\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchtext import data, datasets\n",
    "from nltk import ngrams\n",
    "from torchtext.vocab import GloVe, Vectors\n",
    "from collections import defaultdict\n",
    "data_path = r'D:\\kaggle\\data\\spooky-author-identification\\a'[: -1]\n",
    "data_path_inv = r'D:\\kaggle\\data\\spooky-author-identification\\a'[: -1]\n",
    "data_path_word_vector = r'D:\\kaggle\\data\\word_vector\\a'[: -1]\n",
    "\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 3) (8392, 2) (8392, 4)\n",
      "        id                                               text author\n",
      "0  id26305  This process, however, afforded me no means of...    EAP\n",
      "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
      "2  id11008  In his left hand was a gold snuff box, from wh...    EAP \n",
      "\n",
      "         id                                               text\n",
      "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
      "1  id24541  If a fire wanted fanning, it could readily be ...\n",
      "2  id00134  And when they had broken down the frail door t... \n",
      "\n",
      "         id       EAP       HPL       MWS\n",
      "0  id02310  0.403494  0.287808  0.308698\n",
      "1  id24541  0.403494  0.287808  0.308698\n",
      "2  id00134  0.403494  0.287808  0.308698\n"
     ]
    }
   ],
   "source": [
    "def read_data():\n",
    "    df_train = pd.read_csv(data_path_inv + r'train/train.csv')\n",
    "    df_test = pd.read_csv(data_path + 'test/test.csv')\n",
    "    df_sub = pd.read_csv(data_path + 'sample_submission/sample_submission.csv')\n",
    "    return df_train, df_test, df_sub\n",
    "df_train, df_test, df_sub = read_data()\n",
    "print (df_train.shape, df_test.shape, df_sub.shape)\n",
    "print (df_train.head(3), '\\n\\n', df_test.head(3), '\\n\\n', df_sub.head(3), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n",
      "(8392,)\n"
     ]
    }
   ],
   "source": [
    "# 处理数据\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(df_train.author.values)\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(df_train.text.values, y, \n",
    "                                                  stratify = y, \n",
    "                                                  random_state = 2020, \n",
    "                                                  test_size = 0.1, shuffle = True)\n",
    "xtest = df_test.text.values\n",
    "print (xtrain.shape)\n",
    "print (xvalid.shape)\n",
    "print (xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 500000, time:42.846012592315674\n",
      "index: 1000000, time:40.68779492378235\n",
      "index: 1500000, time:39.82404017448425\n",
      "index: 2000000, time:41.08426308631897\n",
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "embeddings_index = {}\n",
    "f = open(data_path_word_vector + 'glove.840B.300d.txt', 'rb')\n",
    "index = 0\n",
    "pre_time = time.time()\n",
    "for line in f: # tqdm(f):\n",
    "    index += 1\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    if index % 500000 == 0:\n",
    "        print ('index: {:}, time:{:}'.format(index, time.time() - pre_time))\n",
    "        pre_time = time.time()\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,) <class 'numpy.ndarray'> (17,)\n",
      "(1958,) <class 'numpy.ndarray'> (17,)\n",
      "(8392,) <class 'numpy.ndarray'> (17,)\n",
      "(17621,) (1958,)\n",
      "(17621,) (1958,)\n",
      "[1 0 1] [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            torch_tmp = torch.from_numpy(np.array(embeddings_index[str.encode(w)])).cuda().type(torch.float32)\n",
    "            M.append(torch_tmp)\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    # v = M.sum(axis=0)\n",
    "    v = M\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros((1, 300))\n",
    "    return v\n",
    "\n",
    "xtrain_glove = np.array([sent2vec(x) for x in xtrain])\n",
    "print (xtrain_glove.shape, type(xtrain_glove[0]), xtrain_glove[0].shape)\n",
    "xvalid_glove = np.array([sent2vec(x) for x in xvalid])\n",
    "print (xvalid_glove.shape, type(xvalid_glove[0]), xtrain_glove[0].shape)\n",
    "xtest_glove = np.array([sent2vec(x) for x in xtest])\n",
    "print (xtest_glove.shape, type(xtest_glove[0]), xtrain_glove[0].shape)\n",
    "\n",
    "# # scale the data before any neural net:\n",
    "# scl = preprocessing.StandardScaler()\n",
    "# xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "# xvalid_glove_scl = scl.transform(xvalid_glove)\n",
    "\n",
    "print (xtrain_glove.shape, xvalid_glove.shape)\n",
    "print (ytrain.shape, yvalid.shape)\n",
    "print (ytrain[: 3], yvalid[: 3])\n",
    "word_vector_size = len(embeddings_index[list(embeddings_index.keys())[0]])\n",
    "label_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use cuda\n"
     ]
    }
   ],
   "source": [
    "num_inputs, num_hiddens, num_outputs = word_vector_size, 512, 3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('will use', device)\n",
    "\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32)\n",
    "        return torch.nn.Parameter(ts, requires_grad=True)\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = _one((num_inputs, num_hiddens))\n",
    "    W_hh = _one((num_hiddens, num_hiddens))\n",
    "    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device, requires_grad=True))\n",
    "    # 输出层参数\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, requires_grad=True))\n",
    "    return nn.ParameterList([W_xh, W_hh, b_h, W_hq, b_q])\n",
    "\n",
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )\n",
    "\n",
    "def rnn(inputs, state, params):\n",
    "    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)\n",
    "    Y = torch.matmul(H, W_hq) + b_q\n",
    "    # Y = Y.view(-1)\n",
    "    return Y\n",
    "\n",
    "def grad_clipping(params, theta, device):\n",
    "    norm = torch.tensor([0.0], device=device)\n",
    "    for param in params:\n",
    "        norm += (param.grad.data ** 2).sum()\n",
    "    norm = norm.sqrt().item()\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad.data *= (theta / norm)\n",
    "\n",
    "def data_iter_fn(xtrain, ytrain, batch_size, device):\n",
    "    num_steps = len(xtrain_glove_scl) // batch_size\n",
    "    x_out, y_out = [], []\n",
    "    for epoch in range(num_steps):\n",
    "        xval = torch.from_numpy(xtrain[i * batch_size: (i + 1) * batch_size, :]).cuda().type(torch.float32)\n",
    "        yval = ytrain[i * batch_size: (i + 1) * batch_size]\n",
    "        x_out.append(xval)\n",
    "        y_out.append(yval)\n",
    "    y_out = torch.from_numpy(np.array(y_out)).cuda().type(torch.float32)\n",
    "    return x_out, y_out\n",
    "\n",
    "def sgd(params, lr, batch_size):\n",
    "    # 为了和原书保持一致，这里除以了batch_size，但是应该是不用除的，因为一般用PyTorch计算loss时就默认已经\n",
    "    # 沿batch维求了平均了。\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad / batch_size # 注意这里更改param时用的param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, perplexity 2.998735, time 185.71 sec\n",
      "epoch 2, perplexity 3.194917, time 371.93 sec\n",
      "epoch 3, perplexity 3.112331, time 558.72 sec\n",
      "epoch 4, perplexity 3.169388, time 745.13 sec\n",
      "epoch 5, perplexity 3.138139, time 933.07 sec\n",
      "epoch 6, perplexity 3.172320, time 1120.53 sec\n",
      "epoch 7, perplexity 3.159111, time 1309.18 sec\n",
      "epoch 8, perplexity 3.153510, time 1497.60 sec\n",
      "epoch 9, perplexity 3.147549, time 1685.59 sec\n",
      "epoch 10, perplexity 3.146339, time 1874.02 sec\n",
      "epoch 11, perplexity 3.152502, time 2063.05 sec\n",
      "epoch 12, perplexity 3.137123, time 2250.50 sec\n",
      "epoch 13, perplexity 3.142323, time 2439.37 sec\n",
      "epoch 14, perplexity 3.135014, time 2626.43 sec\n",
      "epoch 15, perplexity 3.126536, time 2814.30 sec\n",
      "epoch 16, perplexity 3.115769, time 3000.86 sec\n",
      "epoch 17, perplexity 3.136667, time 3188.19 sec\n",
      "epoch 18, perplexity 3.123915, time 3376.13 sec\n",
      "epoch 19, perplexity 3.138564, time 3563.69 sec\n",
      "epoch 20, perplexity 3.126029, time 3750.41 sec\n",
      "epoch 21, perplexity 3.109821, time 3936.64 sec\n",
      "epoch 22, perplexity 3.167076, time 4123.41 sec\n",
      "epoch 23, perplexity 3.164919, time 4311.56 sec\n",
      "epoch 24, perplexity 3.163670, time 4498.60 sec\n",
      "epoch 25, perplexity 3.158026, time 4685.02 sec\n",
      "epoch 26, perplexity 3.150745, time 4871.30 sec\n",
      "epoch 27, perplexity 3.144100, time 5058.80 sec\n",
      "epoch 28, perplexity 3.141194, time 5246.24 sec\n",
      "epoch 29, perplexity 3.131700, time 5434.00 sec\n",
      "epoch 30, perplexity 3.140211, time 5621.24 sec\n",
      "epoch 31, perplexity 3.134549, time 5809.60 sec\n",
      "epoch 32, perplexity 3.129941, time 5997.87 sec\n",
      "epoch 33, perplexity 3.124389, time 6185.94 sec\n",
      "epoch 34, perplexity 3.117984, time 6373.13 sec\n",
      "epoch 35, perplexity 3.116495, time 6561.03 sec\n",
      "epoch 36, perplexity 3.108711, time 6748.79 sec\n",
      "epoch 37, perplexity 3.124513, time 6936.91 sec\n",
      "epoch 38, perplexity 3.120388, time 7124.65 sec\n",
      "epoch 39, perplexity 3.115282, time 7312.38 sec\n",
      "epoch 40, perplexity 3.110668, time 7500.20 sec\n",
      "epoch 41, perplexity 3.109152, time 7688.15 sec\n",
      "epoch 42, perplexity 3.101614, time 7875.92 sec\n",
      "epoch 43, perplexity 3.105157, time 8062.76 sec\n",
      "epoch 44, perplexity 3.127637, time 8251.25 sec\n",
      "epoch 45, perplexity 3.122707, time 8437.34 sec\n",
      "epoch 46, perplexity 3.118546, time 8624.93 sec\n",
      "epoch 47, perplexity 3.118219, time 8811.89 sec\n",
      "epoch 48, perplexity 3.113657, time 9000.69 sec\n",
      "epoch 49, perplexity 3.109932, time 9188.59 sec\n",
      "epoch 50, perplexity 3.108411, time 9376.54 sec\n",
      "epoch 51, perplexity 3.104251, time 9564.28 sec\n",
      "epoch 52, perplexity 3.097348, time 9752.03 sec\n",
      "epoch 53, perplexity 3.100020, time 9939.00 sec\n",
      "epoch 54, perplexity 3.096074, time 10127.86 sec\n",
      "epoch 55, perplexity 3.089325, time 10316.05 sec\n",
      "epoch 56, perplexity 3.140123, time 10503.94 sec\n",
      "epoch 57, perplexity 3.139509, time 10692.32 sec\n",
      "epoch 58, perplexity 3.136634, time 10881.05 sec\n",
      "epoch 59, perplexity 3.132479, time 11069.17 sec\n",
      "epoch 60, perplexity 3.127798, time 11257.43 sec\n",
      "epoch 61, perplexity 3.132842, time 11445.40 sec\n",
      "epoch 62, perplexity 3.128055, time 11633.78 sec\n",
      "epoch 63, perplexity 3.126900, time 11821.77 sec\n",
      "epoch 64, perplexity 3.123421, time 12009.35 sec\n",
      "epoch 65, perplexity 3.118058, time 12197.84 sec\n",
      "epoch 66, perplexity 3.119069, time 12386.91 sec\n",
      "epoch 67, perplexity 3.114711, time 12574.48 sec\n",
      "epoch 68, perplexity 3.110764, time 12762.94 sec\n",
      "epoch 69, perplexity 3.111322, time 12950.85 sec\n",
      "epoch 70, perplexity 3.108211, time 13138.21 sec\n",
      "epoch 71, perplexity 3.103440, time 13325.95 sec\n",
      "epoch 72, perplexity 3.103005, time 13514.33 sec\n",
      "epoch 73, perplexity 3.101301, time 13702.53 sec\n",
      "epoch 74, perplexity 3.095061, time 13889.91 sec\n",
      "epoch 75, perplexity 3.094117, time 14077.62 sec\n",
      "epoch 76, perplexity 3.097435, time 14266.12 sec\n",
      "epoch 77, perplexity 3.093336, time 14454.99 sec\n",
      "epoch 78, perplexity 3.097135, time 14642.96 sec\n",
      "epoch 79, perplexity 3.091794, time 14831.30 sec\n",
      "epoch 80, perplexity 3.101487, time 15019.57 sec\n",
      "epoch 81, perplexity 3.096650, time 15207.17 sec\n",
      "epoch 82, perplexity 3.103770, time 15395.04 sec\n",
      "epoch 83, perplexity 3.101443, time 15582.89 sec\n",
      "epoch 84, perplexity 3.100132, time 15771.43 sec\n",
      "epoch 85, perplexity 3.096480, time 15958.89 sec\n",
      "epoch 86, perplexity 3.093563, time 16146.50 sec\n",
      "epoch 87, perplexity 3.092661, time 16335.14 sec\n",
      "epoch 88, perplexity 3.091517, time 16523.19 sec\n",
      "epoch 89, perplexity 3.090304, time 16711.40 sec\n",
      "epoch 90, perplexity 3.088211, time 16899.36 sec\n",
      "epoch 91, perplexity 3.085043, time 17087.00 sec\n",
      "epoch 92, perplexity 3.083362, time 17274.69 sec\n",
      "epoch 93, perplexity 3.080517, time 17462.77 sec\n",
      "epoch 94, perplexity 3.075951, time 17649.91 sec\n",
      "epoch 95, perplexity 3.078361, time 17837.24 sec\n",
      "epoch 96, perplexity 3.074490, time 18024.94 sec\n",
      "epoch 97, perplexity 3.085837, time 18212.86 sec\n",
      "epoch 98, perplexity 3.082648, time 18400.79 sec\n",
      "epoch 99, perplexity 3.080468, time 18589.88 sec\n",
      "epoch 100, perplexity 3.080349, time 18776.96 sec\n",
      "epoch 101, perplexity 3.077518, time 18965.07 sec\n",
      "epoch 102, perplexity 3.075208, time 19152.46 sec\n",
      "epoch 103, perplexity 3.073818, time 19340.29 sec\n",
      "epoch 104, perplexity 3.070809, time 19528.46 sec\n",
      "epoch 105, perplexity 3.068249, time 19716.52 sec\n",
      "epoch 106, perplexity 3.066807, time 19904.73 sec\n",
      "epoch 107, perplexity 3.069249, time 20092.58 sec\n",
      "epoch 108, perplexity 3.066778, time 20279.67 sec\n",
      "epoch 109, perplexity 3.063737, time 20467.45 sec\n",
      "epoch 110, perplexity 3.062710, time 20655.87 sec\n",
      "epoch 111, perplexity 3.062541, time 20843.55 sec\n",
      "epoch 112, perplexity 3.060605, time 21030.92 sec\n",
      "epoch 113, perplexity 3.058490, time 21219.14 sec\n",
      "epoch 114, perplexity 3.057668, time 21406.17 sec\n",
      "epoch 115, perplexity 3.053476, time 21594.57 sec\n",
      "epoch 116, perplexity 3.058660, time 21782.30 sec\n",
      "epoch 117, perplexity 3.055324, time 21970.06 sec\n",
      "epoch 118, perplexity 3.053442, time 22157.57 sec\n",
      "epoch 119, perplexity 3.053226, time 22346.05 sec\n",
      "epoch 120, perplexity 3.051434, time 22536.94 sec\n",
      "epoch 121, perplexity 3.049308, time 22725.41 sec\n",
      "epoch 122, perplexity 3.047677, time 22912.38 sec\n",
      "epoch 123, perplexity 3.046410, time 23100.41 sec\n",
      "epoch 124, perplexity 3.043499, time 23288.26 sec\n",
      "epoch 125, perplexity 3.043904, time 23475.47 sec\n",
      "epoch 126, perplexity 3.043594, time 23663.06 sec\n",
      "epoch 127, perplexity 3.041444, time 23851.73 sec\n",
      "epoch 128, perplexity 3.038869, time 24039.67 sec\n",
      "epoch 129, perplexity 3.039333, time 24228.01 sec\n",
      "epoch 130, perplexity 3.037285, time 24416.74 sec\n",
      "epoch 131, perplexity 3.035501, time 24605.45 sec\n",
      "epoch 132, perplexity 3.033678, time 24793.48 sec\n",
      "epoch 133, perplexity 3.030486, time 24984.53 sec\n",
      "epoch 134, perplexity 3.032040, time 25172.89 sec\n",
      "epoch 135, perplexity 3.028951, time 25361.68 sec\n",
      "epoch 136, perplexity 3.040035, time 25550.68 sec\n",
      "epoch 137, perplexity 3.038194, time 25738.75 sec\n",
      "epoch 138, perplexity 3.035959, time 25926.87 sec\n",
      "epoch 139, perplexity 3.033579, time 26116.27 sec\n",
      "epoch 140, perplexity 3.032131, time 26304.42 sec\n",
      "epoch 141, perplexity 3.030793, time 26492.52 sec\n",
      "epoch 142, perplexity 3.029734, time 26680.49 sec\n",
      "epoch 143, perplexity 3.027198, time 26868.60 sec\n",
      "epoch 144, perplexity 3.024914, time 27057.57 sec\n",
      "epoch 145, perplexity 3.022142, time 27245.43 sec\n",
      "epoch 146, perplexity 3.022174, time 27433.31 sec\n",
      "epoch 147, perplexity 3.019207, time 27621.52 sec\n",
      "epoch 148, perplexity 3.023400, time 27809.90 sec\n",
      "epoch 149, perplexity 3.020849, time 27997.95 sec\n",
      "epoch 150, perplexity 3.021249, time 28186.23 sec\n",
      "epoch 151, perplexity 3.018307, time 28374.16 sec\n",
      "epoch 152, perplexity 3.015534, time 28563.49 sec\n",
      "epoch 153, perplexity 3.015466, time 28751.36 sec\n",
      "epoch 154, perplexity 3.013728, time 28939.51 sec\n",
      "epoch 155, perplexity 3.012075, time 29127.48 sec\n",
      "epoch 156, perplexity 3.010513, time 29315.98 sec\n",
      "epoch 157, perplexity 3.009763, time 29504.64 sec\n",
      "epoch 158, perplexity 3.007294, time 29693.08 sec\n",
      "epoch 159, perplexity 3.005316, time 29880.12 sec\n",
      "epoch 160, perplexity 3.005284, time 30068.45 sec\n",
      "epoch 161, perplexity 3.004645, time 30256.36 sec\n",
      "epoch 162, perplexity 3.003255, time 30444.53 sec\n",
      "epoch 163, perplexity 3.001911, time 30632.94 sec\n",
      "epoch 164, perplexity 2.999464, time 30821.30 sec\n",
      "epoch 165, perplexity 3.001761, time 31009.32 sec\n",
      "epoch 166, perplexity 3.000407, time 31197.24 sec\n",
      "epoch 167, perplexity 2.998976, time 31386.43 sec\n",
      "epoch 168, perplexity 2.997329, time 31575.36 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 169, perplexity 2.994451, time 31763.86 sec\n",
      "epoch 170, perplexity 2.992747, time 31951.72 sec\n",
      "epoch 171, perplexity 2.995761, time 32140.44 sec\n",
      "epoch 172, perplexity 2.995254, time 32328.59 sec\n",
      "epoch 173, perplexity 2.993777, time 32517.26 sec\n",
      "epoch 174, perplexity 2.992090, time 32705.79 sec\n",
      "epoch 175, perplexity 2.991461, time 32895.33 sec\n",
      "epoch 176, perplexity 2.989060, time 33082.98 sec\n",
      "epoch 177, perplexity 2.986736, time 33271.23 sec\n",
      "epoch 178, perplexity 2.986170, time 33459.51 sec\n",
      "epoch 179, perplexity 2.992123, time 33648.43 sec\n",
      "epoch 180, perplexity 2.991777, time 33837.09 sec\n",
      "epoch 181, perplexity 2.989246, time 34024.48 sec\n",
      "epoch 182, perplexity 2.988164, time 34212.74 sec\n",
      "epoch 183, perplexity 2.987287, time 34400.24 sec\n",
      "epoch 184, perplexity 2.985100, time 34588.51 sec\n",
      "epoch 185, perplexity 2.983016, time 34776.20 sec\n",
      "epoch 186, perplexity 2.981372, time 34963.19 sec\n",
      "epoch 187, perplexity 2.979879, time 35151.43 sec\n",
      "epoch 188, perplexity 2.979917, time 35339.73 sec\n",
      "epoch 189, perplexity 2.979227, time 35527.47 sec\n",
      "epoch 190, perplexity 2.978349, time 35715.56 sec\n",
      "epoch 191, perplexity 2.977225, time 35904.35 sec\n",
      "epoch 192, perplexity 2.977632, time 36092.13 sec\n",
      "epoch 193, perplexity 2.975593, time 36279.88 sec\n",
      "epoch 194, perplexity 2.974363, time 36467.80 sec\n",
      "epoch 195, perplexity 2.972340, time 36656.66 sec\n",
      "epoch 196, perplexity 2.972007, time 36845.38 sec\n",
      "epoch 197, perplexity 2.971721, time 37033.35 sec\n",
      "epoch 198, perplexity 2.970066, time 37221.68 sec\n",
      "epoch 199, perplexity 2.967913, time 37409.90 sec\n",
      "epoch 200, perplexity 2.967832, time 37597.78 sec\n"
     ]
    }
   ],
   "source": [
    "# 定义模型训练函数\n",
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "def train(xtrain, ytrain, rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      word_vector_size, device, num_epochs, lr, clipping_theta, batch_size):\n",
    "    params = get_params()\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    l_sum, n, start = 0.0, 0, time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        outputs = []\n",
    "        for X in xtrain:\n",
    "            state = init_rnn_state(batch_size, num_hiddens, device)\n",
    "            Y = rnn(X, state, params)\n",
    "            # val = Y.cpu().detach().numpy().reshape(-1)\n",
    "            # print (Y.shape)\n",
    "            outputs.append(Y)\n",
    "        outputs = torch.cat(outputs, dim=0)\n",
    "        # Y的形状是(batch_size, num_steps)，转置后再变成长度为\n",
    "        # batch * num_steps 的向量，这样跟输出的行一一对应\n",
    "        y = torch.from_numpy(ytrain).cuda().type(torch.float32)\n",
    "        l = loss(outputs, y.long())\n",
    "\n",
    "        # 梯度清0\n",
    "        if params[0].grad is not None:\n",
    "            for param in params:\n",
    "                param.grad.data.zero_()\n",
    "        l.backward()\n",
    "        grad_clipping(params, clipping_theta, device)  # 裁剪梯度\n",
    "        sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均\n",
    "        l_sum += l.item() * y.shape[0]\n",
    "        n += y.shape[0]\n",
    "        \n",
    "        pred_period = 1\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "                epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "    return params\n",
    "\n",
    "# 训练模型\n",
    "num_epochs, lr, clipping_theta, batch_size = 200, 1e2, 1e-2, 1\n",
    "params = train(xtrain_glove, ytrain, rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      word_vector_size, device, num_epochs, lr, clipping_theta, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1958,) (1958,)\n",
      "acc : 839 / 1958 = 0.428\n"
     ]
    }
   ],
   "source": [
    "def get_outputs(inputs, params):\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        state = init_rnn_state(batch_size, num_hiddens, device)\n",
    "        Y = rnn(X, state, params)\n",
    "        outputs.append(Y)\n",
    "    outputs = torch.cat(outputs, dim=0)\n",
    "    return outputs\n",
    "\n",
    "def validation(xvalidation, yvalidation, params):\n",
    "    outputs = get_outputs(xvalidation, params)\n",
    "    y_pred = torch.max(outputs, 1)[1].cpu().numpy()\n",
    "    y_val = yvalidation\n",
    "    print (y_pred.shape, y_val.shape)\n",
    "    cnt_all = len(y_pred)\n",
    "    cnt_correct = (y_pred == y_val).sum()\n",
    "    print (\"acc : {:} / {:} = {:.3f}\".format(cnt_correct, cnt_all, cnt_correct * 1.0 / cnt_all))\n",
    "\n",
    "validation(xvalid_glove, yvalid, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8392 8392\n"
     ]
    }
   ],
   "source": [
    "outputs = get_outputs(xtest_glove, params)\n",
    "y_out = F.softmax(outputs, dim = 1).cpu().detach().numpy()\n",
    "df_sub_out = pd.DataFrame(y_out, columns = ['EAP', 'HPL', 'MWS'])\n",
    "df_sub_out['id'] = df_test['id']\n",
    "df_sub_out[['id', 'EAP', 'HPL', 'MWS']].to_csv(data_path_inv + 'submission.csv', index = False, header = True)\n",
    "print (len(df_sub_out), len(df_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        EAP       HPL       MWS       id\n",
      "0  0.404067  0.274253  0.321680  id02310\n",
      "1  0.188901  0.217563  0.593536  id24541\n",
      "2  0.443464  0.516451  0.040085  id00134         id       EAP       HPL       MWS\n",
      "0  id02310  0.403494  0.287808  0.308698\n",
      "1  id24541  0.403494  0.287808  0.308698\n",
      "2  id00134  0.403494  0.287808  0.308698\n"
     ]
    }
   ],
   "source": [
    "print (df_sub_out.head(3), df_sub.head(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
