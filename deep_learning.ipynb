{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding:utf-8\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "from sklearn.externals import joblib\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from glob import glob\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "import shutil\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchtext import data, datasets\n",
    "from nltk import ngrams\n",
    "from torchtext.vocab import GloVe, Vectors\n",
    "from collections import defaultdict\n",
    "data_path = r'D:\\kaggle\\data\\spooky-author-identification\\a'[: -1]\n",
    "data_path_inv = r'D:\\kaggle\\data\\spooky-author-identification\\a'[: -1]\n",
    "data_path_word_vector = r'D:\\kaggle\\data\\word_vector\\a'[: -1]\n",
    "\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 3) (8392, 2) (8392, 4)\n",
      "        id                                               text author\n",
      "0  id26305  This process, however, afforded me no means of...    EAP\n",
      "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
      "2  id11008  In his left hand was a gold snuff box, from wh...    EAP \n",
      "\n",
      "         id                                               text\n",
      "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
      "1  id24541  If a fire wanted fanning, it could readily be ...\n",
      "2  id00134  And when they had broken down the frail door t... \n",
      "\n",
      "         id       EAP       HPL       MWS\n",
      "0  id02310  0.403494  0.287808  0.308698\n",
      "1  id24541  0.403494  0.287808  0.308698\n",
      "2  id00134  0.403494  0.287808  0.308698\n"
     ]
    }
   ],
   "source": [
    "def read_data():\n",
    "    df_train = pd.read_csv(data_path_inv + r'train/train.csv')\n",
    "    df_test = pd.read_csv(data_path + 'test/test.csv')\n",
    "    df_sub = pd.read_csv(data_path + 'sample_submission/sample_submission.csv')\n",
    "    return df_train, df_test, df_sub\n",
    "df_train, df_test, df_sub = read_data()\n",
    "print (df_train.shape, df_test.shape, df_sub.shape)\n",
    "print (df_train.head(3), '\\n\\n', df_test.head(3), '\\n\\n', df_sub.head(3), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n",
      "(8392,)\n"
     ]
    }
   ],
   "source": [
    "# 处理数据\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(df_train.author.values)\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(df_train.text.values, y, \n",
    "                                                  stratify = y, \n",
    "                                                  random_state = 2020, \n",
    "                                                  test_size = 0.1, shuffle = True)\n",
    "xtest = df_test.text.values\n",
    "print (xtrain.shape)\n",
    "print (xvalid.shape)\n",
    "print (xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 100000, time:8.498717784881592\n",
      "index: 200000, time:8.46131181716919\n",
      "index: 300000, time:8.484855890274048\n",
      "index: 400000, time:8.412349224090576\n",
      "index: 500000, time:8.444247245788574\n",
      "index: 600000, time:8.373737335205078\n",
      "index: 700000, time:8.501246452331543\n",
      "index: 800000, time:8.392608880996704\n",
      "index: 900000, time:8.472589254379272\n",
      "index: 1000000, time:8.454006671905518\n",
      "index: 1100000, time:8.509315252304077\n",
      "index: 1200000, time:8.338173866271973\n",
      "index: 1300000, time:8.497522830963135\n",
      "index: 1400000, time:8.592368841171265\n",
      "index: 1500000, time:8.432148456573486\n",
      "index: 1600000, time:8.278294324874878\n",
      "index: 1700000, time:8.268243312835693\n",
      "index: 1800000, time:8.38856053352356\n",
      "index: 1900000, time:8.368398427963257\n",
      "index: 2000000, time:8.25022554397583\n",
      "index: 2100000, time:8.356012105941772\n",
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "embeddings_index = {}\n",
    "f = open(data_path_word_vector + 'glove.840B.300d.txt', 'rb')\n",
    "index = 0\n",
    "pre_time = time.time()\n",
    "for line in f: # tqdm(f):\n",
    "    index += 1\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    if index % 500000 == 0:\n",
    "        print ('index: {:}, time:{:}'.format(index, time.time() - pre_time))\n",
    "        pre_time = time.time()\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,) <class 'numpy.ndarray'> (17,)\n",
      "(1958,) <class 'numpy.ndarray'> (17,)\n",
      "(8392,) <class 'numpy.ndarray'> (17,)\n",
      "(17621,) (1958,)\n",
      "(17621,) (1958,)\n",
      "[1 0 1] [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            torch_tmp = torch.from_numpy(np.array(embeddings_index[str.encode(w)])).cuda().type(torch.float32)\n",
    "            M.append(torch_tmp)\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    # v = M.sum(axis=0)\n",
    "    v = M\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros((1, 300))\n",
    "    return v\n",
    "\n",
    "xtrain_glove = np.array([sent2vec(x) for x in xtrain])\n",
    "print (xtrain_glove.shape, type(xtrain_glove[0]), xtrain_glove[0].shape)\n",
    "xvalid_glove = np.array([sent2vec(x) for x in xvalid])\n",
    "print (xvalid_glove.shape, type(xvalid_glove[0]), xtrain_glove[0].shape)\n",
    "xtest_glove = np.array([sent2vec(x) for x in xtest])\n",
    "print (xtest_glove.shape, type(xtest_glove[0]), xtrain_glove[0].shape)\n",
    "\n",
    "# # scale the data before any neural net:\n",
    "# scl = preprocessing.StandardScaler()\n",
    "# xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "# xvalid_glove_scl = scl.transform(xvalid_glove)\n",
    "\n",
    "print (xtrain_glove.shape, xvalid_glove.shape)\n",
    "print (ytrain.shape, yvalid.shape)\n",
    "print (ytrain[: 3], yvalid[: 3])\n",
    "word_vector_size = len(embeddings_index[list(embeddings_index.keys())[0]])\n",
    "label_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use cuda\n"
     ]
    }
   ],
   "source": [
    "num_inputs, num_hiddens, num_outputs = word_vector_size, 512, 3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('will use', device)\n",
    "\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32)\n",
    "        return torch.nn.Parameter(ts, requires_grad=True)\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = _one((num_inputs, num_hiddens))\n",
    "    W_hh = _one((num_hiddens, num_hiddens))\n",
    "    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device, requires_grad=True))\n",
    "    # 输出层参数\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, requires_grad=True))\n",
    "    return nn.ParameterList([W_xh, W_hh, b_h, W_hq, b_q])\n",
    "\n",
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )\n",
    "\n",
    "def rnn(inputs, state, params):\n",
    "    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)\n",
    "    Y = torch.matmul(H, W_hq) + b_q\n",
    "    # Y = Y.view(-1)\n",
    "    return Y\n",
    "\n",
    "def grad_clipping(params, theta, device):\n",
    "    norm = torch.tensor([0.0], device=device)\n",
    "    for param in params:\n",
    "        norm += (param.grad.data ** 2).sum()\n",
    "    norm = norm.sqrt().item()\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad.data *= (theta / norm)\n",
    "\n",
    "def data_iter_fn(xtrain, ytrain, batch_size, device):\n",
    "    num_steps = len(xtrain_glove_scl) // batch_size\n",
    "    x_out, y_out = [], []\n",
    "    for epoch in range(num_steps):\n",
    "        xval = torch.from_numpy(xtrain[i * batch_size: (i + 1) * batch_size, :]).cuda().type(torch.float32)\n",
    "        yval = ytrain[i * batch_size: (i + 1) * batch_size]\n",
    "        x_out.append(xval)\n",
    "        y_out.append(yval)\n",
    "    y_out = torch.from_numpy(np.array(y_out)).cuda().type(torch.float32)\n",
    "    return x_out, y_out\n",
    "\n",
    "def sgd(params, lr, batch_size):\n",
    "    # 为了和原书保持一致，这里除以了batch_size，但是应该是不用除的，因为一般用PyTorch计算loss时就默认已经\n",
    "    # 沿batch维求了平均了。\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad / batch_size # 注意这里更改param时用的param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, perplexity 3.001179, time 201.09 sec\n"
     ]
    }
   ],
   "source": [
    "# 定义模型训练函数\n",
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "def train(xtrain, ytrain, rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      word_vector_size, device, num_epochs, lr, clipping_theta, batch_size):\n",
    "    params = get_params()\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    l_sum, n, start = 0.0, 0, time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        outputs = []\n",
    "        for X in xtrain:\n",
    "            state = init_rnn_state(batch_size, num_hiddens, device)\n",
    "            Y = rnn(X, state, params)\n",
    "            # val = Y.cpu().detach().numpy().reshape(-1)\n",
    "            # print (Y.shape)\n",
    "            outputs.append(Y)\n",
    "        outputs = torch.cat(outputs, dim=0)\n",
    "        # Y的形状是(batch_size, num_steps)，转置后再变成长度为\n",
    "        # batch * num_steps 的向量，这样跟输出的行一一对应\n",
    "        y = torch.from_numpy(ytrain).cuda().type(torch.float32)\n",
    "        l = loss(outputs, y.long())\n",
    "\n",
    "        # 梯度清0\n",
    "        if params[0].grad is not None:\n",
    "            for param in params:\n",
    "                param.grad.data.zero_()\n",
    "        l.backward()\n",
    "        grad_clipping(params, clipping_theta, device)  # 裁剪梯度\n",
    "        sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均\n",
    "        l_sum += l.item() * y.shape[0]\n",
    "        n += y.shape[0]\n",
    "        \n",
    "        pred_period = 1\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "                epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "    return params\n",
    "\n",
    "# 训练模型\n",
    "num_epochs, lr, clipping_theta, batch_size = 15, 1e2, 1e-2, 1\n",
    "params = train(xtrain_glove, ytrain, rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      word_vector_size, device, num_epochs, lr, clipping_theta, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputs(inputs, params):\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        state = init_rnn_state(batch_size, num_hiddens, device)\n",
    "        Y = rnn(X, state, params)\n",
    "        outputs.append(Y)\n",
    "    outputs = torch.cat(outputs, dim=0)\n",
    "    return outputs\n",
    "\n",
    "def validation(xvalidation, yvalidation, params):\n",
    "    outputs = get_outputs(xvalidation, params)\n",
    "    y_pred = torch.max(outputs, 1)[1].cpu().numpy()\n",
    "    y_val = yvalidation\n",
    "    print (y_pred.shape, y_val.shape)\n",
    "    cnt_all = len(y_pred)\n",
    "    cnt_correct = (y_pred == y_val).sum()\n",
    "    print (\"acc : {:} / {:} = {:.3f}\".format(cnt_correct, cnt_all, cnt_correct * 1.0 / cnt_all))\n",
    "\n",
    "validation(xvalid_glove, yvalid, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = get_outputs(xtest_glove, params)\n",
    "y_out = F.softmax(outputs, dim = 1).cpu().detach().numpy()\n",
    "df_sub_out = pd.DataFrame(y_out, columns = ['EAP', 'HPL', 'MWS'])\n",
    "df_sub_out['id'] = df_test['id']\n",
    "df_sub_out[['id', 'EAP', 'HPL', 'MWS']].to_csv(data_path_inv + 'submission.csv', index = False, header = True)\n",
    "print (len(df_sub_out), len(df_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_sub_out.head(3), df_sub.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
