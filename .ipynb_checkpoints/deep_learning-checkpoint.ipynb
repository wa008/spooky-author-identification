{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding:utf-8\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "from sklearn.externals import joblib\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from glob import glob\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "import shutil\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchtext import data, datasets\n",
    "from nltk import ngrams\n",
    "from torchtext.vocab import GloVe, Vectors\n",
    "from collections import defaultdict\n",
    "data_path = r'D:\\kaggle\\data\\spooky-author-identification\\a'[: -1]\n",
    "data_path_inv = r'D:\\kaggle\\data\\spooky-author-identification\\a'[: -1]\n",
    "data_path_word_vector = r'D:\\kaggle\\data\\word_vector\\a'[: -1]\n",
    "\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 3) (8392, 2) (8392, 4)\n",
      "        id                                               text author\n",
      "0  id26305  This process, however, afforded me no means of...    EAP\n",
      "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
      "2  id11008  In his left hand was a gold snuff box, from wh...    EAP \n",
      "\n",
      "         id                                               text\n",
      "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
      "1  id24541  If a fire wanted fanning, it could readily be ...\n",
      "2  id00134  And when they had broken down the frail door t... \n",
      "\n",
      "         id       EAP       HPL       MWS\n",
      "0  id02310  0.403494  0.287808  0.308698\n",
      "1  id24541  0.403494  0.287808  0.308698\n",
      "2  id00134  0.403494  0.287808  0.308698\n"
     ]
    }
   ],
   "source": [
    "def read_data():\n",
    "    df_train = pd.read_csv(data_path_inv + r'train/train.csv')\n",
    "    df_test = pd.read_csv(data_path + 'test/test.csv')\n",
    "    df_sub = pd.read_csv(data_path + 'sample_submission/sample_submission.csv')\n",
    "    return df_train, df_test, df_sub\n",
    "df_train, df_test, df_sub = read_data()\n",
    "print (df_train.shape, df_test.shape, df_sub.shape)\n",
    "print (df_train.head(3), '\\n\\n', df_test.head(3), '\\n\\n', df_sub.head(3), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n"
     ]
    }
   ],
   "source": [
    "# 处理数据\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(df_train.author.values)\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(df_train.text.values, y, \n",
    "                                                  stratify = y, \n",
    "                                                  random_state = 2020, \n",
    "                                                  test_size = 0.1, shuffle = True)\n",
    "print (xtrain.shape)\n",
    "print (xvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 100000, time:8.408922910690308\n",
      "index: 200000, time:8.416667699813843\n",
      "index: 300000, time:8.728966236114502\n",
      "index: 400000, time:8.552128791809082\n",
      "index: 500000, time:8.418014526367188\n",
      "index: 600000, time:8.323860168457031\n",
      "index: 700000, time:8.43789529800415\n",
      "index: 800000, time:8.315868139266968\n",
      "index: 900000, time:8.492916345596313\n",
      "index: 1000000, time:8.289854049682617\n",
      "index: 1100000, time:8.489907264709473\n",
      "index: 1200000, time:8.489906549453735\n",
      "index: 1300000, time:8.45667552947998\n",
      "index: 1400000, time:8.634939432144165\n",
      "index: 1500000, time:8.336873054504395\n",
      "index: 1600000, time:8.400887250900269\n",
      "index: 1700000, time:8.673804759979248\n",
      "index: 1800000, time:8.64094090461731\n",
      "index: 1900000, time:8.495907545089722\n",
      "index: 2000000, time:8.45525598526001\n",
      "index: 2100000, time:8.463258504867554\n",
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "embeddings_index = {}\n",
    "f = open(data_path_word_vector + 'glove.840B.300d.txt', 'rb')\n",
    "index = 0\n",
    "pre_time = time.time()\n",
    "for line in f: # tqdm(f):\n",
    "    index += 1\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    if index % 100000 == 0:\n",
    "        print ('index: {:}, time:{:}'.format(index, time.time() - pre_time))\n",
    "        pre_time = time.time()\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621, 300) (1958, 300)\n",
      "(17621,) (1958,)\n",
      "[1 0 1] [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "xtrain_glove = np.array([sent2vec(x) for x in xtrain])\n",
    "xvalid_glove = np.array([sent2vec(x) for x in xvalid])\n",
    "\n",
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)\n",
    "\n",
    "print (xtrain_glove_scl.shape, xvalid_glove_scl.shape)\n",
    "print (ytrain.shape, yvalid.shape)\n",
    "print (ytrain[: 3], yvalid[: 3])\n",
    "word_vector_size = len(embeddings_index[list(embeddings_index.keys())[0]])\n",
    "label_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use cuda\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型参数\n",
    "num_inputs, num_hiddens, num_outputs = word_vector_size, 512, label_size\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print ('will use', device)\n",
    "\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32)\n",
    "        return torch.nn.Parameter(ts, requires_grad=True)\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = _one((num_inputs, num_hiddens))\n",
    "    W_hh = _one((num_hiddens, num_hiddens))\n",
    "    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device, requires_grad=True))\n",
    "    # 输出层参数\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, requires_grad=True))\n",
    "    return nn.ParameterList([W_xh, W_hh, b_h, W_hq, b_q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n",
      "torch.Size([100, 3]) torch.Size([100, 512])\n"
     ]
    }
   ],
   "source": [
    "# 定义模型\n",
    "batch_size = 100\n",
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )\n",
    "\n",
    "def rnn(inputs, state, params):\n",
    "    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "#         print (type(X), type(W_xh), type(H), type(W_hh), type(b_h))\n",
    "#         print (X.size(), W_xh.size(), H.size(), W_hh.size(), b_h.size())\n",
    "#         print (X.device, W_xh.device, H.device, W_hh.device, b_h.device)\n",
    "#         print (X.dtype, W_xh.dtype, H.dtype, W_hh.dtype, b_h.dtype)\n",
    "        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)\n",
    "        Y = torch.matmul(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs, (H,)\n",
    "\n",
    "state = init_rnn_state(batch_size = batch_size, num_hiddens = num_hiddens, device = device)\n",
    "inputs = []\n",
    "num_epochs = len(xtrain_glove_scl) // batch_size\n",
    "for i in range(num_epochs):\n",
    "    inputs.append(torch.from_numpy(xtrain_glove_scl[i * batch_size: (i + 1) * batch_size, :]).cuda().type(torch.float32))\n",
    "params = get_params()\n",
    "outputs, state_new = rnn(inputs, state, params)\n",
    "print (len(outputs))\n",
    "print (outputs[0].shape, state_new[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 剪裁梯度\n",
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "def grad_clipping(params, theta, device):\n",
    "    norm = torch.tensor([0.0], device=device)\n",
    "    for param in params:\n",
    "        norm += (param.grad.data ** 2).sum()\n",
    "    norm = norm.sqrt().item()\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad.data *= (theta / norm)\n",
    "\n",
    "def data_iter_fn(xtrain, ytrain, batch_size, device):\n",
    "    num_steps = len(xtrain_glove_scl) // batch_size\n",
    "    x_out, y_out = [], []\n",
    "    for epoch in range(num_steps):\n",
    "        xval = torch.from_numpy(xtrain[i * batch_size: (i + 1) * batch_size, :]).cuda().type(torch.float32)\n",
    "        # yval = torch.from_numpy(ytrain[i * batch_size: (i + 1) * batch_size]).cuda().type(torch.float32)\n",
    "        yval = ytrain[i * batch_size: (i + 1) * batch_size]\n",
    "        x_out.append(xval)\n",
    "        y_out.append(yval)\n",
    "    y_out = torch.from_numpy(np.array(y_out)).cuda().type(torch.float32)\n",
    "    return x_out, y_out\n",
    "\n",
    "def sgd(params, lr, batch_size):\n",
    "    # 为了和原书保持一致，这里除以了batch_size，但是应该是不用除的，因为一般用PyTorch计算loss时就默认已经\n",
    "    # 沿batch维求了平均了。\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad / batch_size # 注意这里更改param时用的param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 3.278684, time 0.25 sec\n",
      "epoch 100, perplexity 3.279104, time 0.26 sec\n",
      "epoch 150, perplexity 3.279317, time 0.25 sec\n",
      "epoch 200, perplexity 3.279459, time 0.25 sec\n"
     ]
    }
   ],
   "source": [
    "# 定义模型训练函数\n",
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "def train_and_predict_rnn(xtrain, ytrain, rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      word_vector_size, device, num_epochs, lr, clipping_theta, batch_size):\n",
    "    params = get_params()\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    state = init_rnn_state(batch_size, num_hiddens, device)\n",
    "    for epoch in range(num_epochs):\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        for s in state:\n",
    "            s.detach_()\n",
    "        X, Y = data_iter_fn(xtrain, ytrain, batch_size, device)\n",
    "        inputs = X\n",
    "#         print (type(inputs), type(inputs))\n",
    "#         print (inputs[0].size())\n",
    "        # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "        (outputs, state) = rnn(inputs, state, params)\n",
    "#         print (type(outputs), len(outputs), outputs[0].size())\n",
    "        # 拼接之后形状为(num_steps * batch_size, vocab_size)\n",
    "        outputs = torch.cat(outputs, dim=0)\n",
    "        # Y的形状是(batch_size, num_steps)，转置后再变成长度为\n",
    "        # batch * num_steps 的向量，这样跟输出的行一一对应\n",
    "        y = torch.transpose(Y, 0, 1).contiguous().view(-1)\n",
    "        # 使用交叉熵损失计算平均分类误差\n",
    "        l = loss(outputs, y.long())\n",
    "\n",
    "        # 梯度清0\n",
    "        if params[0].grad is not None:\n",
    "            for param in params:\n",
    "                param.grad.data.zero_()\n",
    "        l.backward()\n",
    "        grad_clipping(params, clipping_theta, device)  # 裁剪梯度\n",
    "        sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均\n",
    "        l_sum += l.item() * y.shape[0]\n",
    "        n += y.shape[0]\n",
    "\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "                epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "\n",
    "# 训练模型\n",
    "num_epochs, lr, clipping_theta = 250, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']\n",
    "train_and_predict_rnn(xtrain_glove_scl, ytrain, rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      word_vector_size, device, num_epochs, lr, clipping_theta, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2], dtype=torch.int32)\n",
      "tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "y = torch.from_numpy(np.arange(3))\n",
    "print (y)\n",
    "print (y.long())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
