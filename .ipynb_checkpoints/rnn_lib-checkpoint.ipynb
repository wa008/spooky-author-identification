{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding:utf-8\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "from sklearn.externals import joblib\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from glob import glob\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "import shutil\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchtext import data, datasets\n",
    "from nltk import ngrams\n",
    "from torchtext.vocab import GloVe, Vectors\n",
    "from collections import defaultdict\n",
    "data_path = r'D:\\kaggle\\data\\spooky-author-identification\\a'[: -1]\n",
    "data_path_inv = r'D:\\kaggle\\data\\spooky-author-identification\\a'[: -1]\n",
    "data_path_word_vector = r'D:\\kaggle\\data\\word_vector\\a'[: -1]\n",
    "\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "stop_words = stopwords.words('english')\n",
    "from torch.nn import utils as nn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 3) (8392, 2) (8392, 4)\n",
      "        id                                               text author\n",
      "0  id26305  This process, however, afforded me no means of...    EAP\n",
      "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
      "2  id11008  In his left hand was a gold snuff box, from wh...    EAP \n",
      "\n",
      "         id                                               text\n",
      "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
      "1  id24541  If a fire wanted fanning, it could readily be ...\n",
      "2  id00134  And when they had broken down the frail door t... \n",
      "\n",
      "         id       EAP       HPL       MWS\n",
      "0  id02310  0.403494  0.287808  0.308698\n",
      "1  id24541  0.403494  0.287808  0.308698\n",
      "2  id00134  0.403494  0.287808  0.308698\n"
     ]
    }
   ],
   "source": [
    "def read_data():\n",
    "    df_train = pd.read_csv(data_path_inv + r'train/train.csv')\n",
    "    df_test = pd.read_csv(data_path + 'test/test.csv')\n",
    "    df_sub = pd.read_csv(data_path + 'sample_submission/sample_submission.csv')\n",
    "    return df_train, df_test, df_sub\n",
    "df_train, df_test, df_sub = read_data()\n",
    "print (df_train.shape, df_test.shape, df_sub.shape)\n",
    "print (df_train.head(3), '\\n\\n', df_test.head(3), '\\n\\n', df_sub.head(3), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n",
      "(8392,)\n"
     ]
    }
   ],
   "source": [
    "# 处理数据\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(df_train.author.values)\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(df_train.text.values, y, \n",
    "                                                  stratify = y, \n",
    "                                                  random_state = 2020, \n",
    "                                                  test_size = 0.1, shuffle = True)\n",
    "xtest = df_test.text.values\n",
    "print (xtrain.shape)\n",
    "print (xvalid.shape)\n",
    "print (xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 500000, time:41.491108894348145\n",
      "index: 1000000, time:40.718759059906006\n",
      "index: 1500000, time:42.45925521850586\n",
      "index: 2000000, time:42.93292021751404\n",
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "embeddings_index = {}\n",
    "f = open(data_path_word_vector + 'glove.840B.300d.txt', 'rb')\n",
    "index = 0\n",
    "pre_time = time.time()\n",
    "for line in f: # tqdm(f):\n",
    "    index += 1\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    if index % 500000 == 0:\n",
    "        print ('index: {:}, time:{:}'.format(index, time.time() - pre_time))\n",
    "        pre_time = time.time()\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n",
      "252\n",
      "(17621, 252, 300)\n",
      "(1958,)\n",
      "(8392,)\n",
      "(17621, 252, 300) (1958,)\n",
      "(17621,) (1958,)\n",
      "[1 0 1] [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            torch_tmp = embeddings_index[str.encode(w)]\n",
    "            M.append(torch_tmp)\n",
    "        except:\n",
    "            continue\n",
    "    return M\n",
    "\n",
    "xtrain_glove = [sent2vec(x) for x in xtrain]\n",
    "len_max = 0\n",
    "for x in xtrain_glove:\n",
    "    len_max = max(len_max, len(x))\n",
    "print (len_max)\n",
    "for i in range(len(xtrain_glove)):\n",
    "    for j in range(len_max - len(xtrain_glove[i])):\n",
    "        xtrain_glove[i].append(np.zeros(300))\n",
    "len_min = len_max\n",
    "for x in xtrain_glove:\n",
    "    len_min = min(len_min, len(x)) \n",
    "print (len_min)\n",
    "\n",
    "xtrain_glove = np.array(xtrain_glove)\n",
    "print (xtrain_glove.shape)\n",
    "xvalid_glove = np.array([sent2vec(x) for x in xvalid])\n",
    "print (xvalid_glove.shape)\n",
    "xtest_glove = np.array([sent2vec(x) for x in xtest])\n",
    "print (xtest_glove.shape)\n",
    "\n",
    "# # scale the data before any neural net:\n",
    "# scl = preprocessing.StandardScaler()\n",
    "# xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "# xvalid_glove_scl = scl.transform(xvalid_glove)\n",
    "\n",
    "print (xtrain_glove.shape, xvalid_glove.shape)\n",
    "print (ytrain.shape, yvalid.shape)\n",
    "print (ytrain[: 3], yvalid[: 3])\n",
    "word_vector_size = len(embeddings_index[list(embeddings_index.keys())[0]])\n",
    "label_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use cuda\n"
     ]
    }
   ],
   "source": [
    "num_inputs, num_hiddens, num_outputs = word_vector_size, 512, 3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('will use', device)\n",
    "\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32)\n",
    "        return torch.nn.Parameter(ts, requires_grad=True)\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = _one((num_inputs, num_hiddens))\n",
    "    W_hh = _one((num_hiddens, num_hiddens))\n",
    "    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device, requires_grad=True))\n",
    "    # 输出层参数\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, requires_grad=True))\n",
    "    return nn.ParameterList([W_xh, W_hh, b_h, W_hq, b_q])\n",
    "\n",
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )\n",
    "\n",
    "def rnn(inputs, state, params):\n",
    "    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)\n",
    "    Y = torch.matmul(H, W_hq) + b_q\n",
    "    # Y = Y.view(-1)\n",
    "    return Y\n",
    "\n",
    "def grad_clipping(params, theta, device):\n",
    "    norm = torch.tensor([0.0], device=device)\n",
    "    for param in params:\n",
    "        norm += (param.grad.data ** 2).sum()\n",
    "    norm = norm.sqrt().item()\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad.data *= (theta / norm)\n",
    "\n",
    "def data_iter_fn(xtrain, ytrain, batch_size, device):\n",
    "    num_steps = len(xtrain_glove_scl) // batch_size\n",
    "    x_out, y_out = [], []\n",
    "    for epoch in range(num_steps):\n",
    "        xval = torch.from_numpy(xtrain[i * batch_size: (i + 1) * batch_size, :]).cuda().type(torch.float32)\n",
    "        yval = ytrain[i * batch_size: (i + 1) * batch_size]\n",
    "        x_out.append(xval)\n",
    "        y_out.append(yval)\n",
    "    y_out = torch.from_numpy(np.array(y_out)).cuda().type(torch.float32)\n",
    "    return x_out, y_out\n",
    "\n",
    "def sgd(params, lr, batch_size):\n",
    "    # 为了和原书保持一致，这里除以了batch_size，但是应该是不用除的，因为一般用PyTorch计算loss时就默认已经\n",
    "    # 沿batch维求了平均了。\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad / batch_size # 注意这里更改param时用的param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(252, device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 3GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:201",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-2e6bd8422e01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mxtrain_lengths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mxtrain_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain_glove\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseqlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain_glove\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxtrain_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mseqlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseqlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 3GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:201"
     ]
    }
   ],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            torch_tmp = list(embeddings_index[str.encode(w)])\n",
    "            M.append(torch_tmp)\n",
    "        except:\n",
    "            continue\n",
    "    return M\n",
    "\n",
    "xtrain_glove = [sent2vec(x) for x in xtrain]\n",
    "xvalid_glove = [sent2vec(x) for x in xvalid]\n",
    "xtest_glove = [sent2vec(x) for x in xtest]\n",
    "\n",
    "xtrain_lengths = torch.from_numpy(np.array([len(x) for x in xtrain_glove])).cuda()\n",
    "print (xtrain_lengths.max())\n",
    "max_length = 100\n",
    "xtrain_tensor = torch.zeros((len(xtrain_glove), max_length, 300)).long().cuda()\n",
    "for idx, (seq, seqlen) in enumerate(zip(xtrain_glove, xtrain_lengths)):\n",
    "    seqlen = min(seqlen, max_length)\n",
    "    xtrain_tensor[idx, :seqlen] = torch.LongTensor(seq[: seqlen, :])\n",
    "\n",
    "print (typ(xtrain_tersor), xtrain_tersor.size())\n",
    "word_vector_size = len(embeddings_index[list(embeddings_index.keys())[0]])\n",
    "label_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(xtrain_glove), xtrain_glove.shape)\n",
    "len_min = 10000000\n",
    "len_max = 0\n",
    "for i in range(len(xtrain_glove)):\n",
    "    len_min = min(len_min, len(xtrain_glove[i]))\n",
    "    len_max = max(len_max, len(xtrain_glove[i]))\n",
    "    # print (type(xtrain_glove[i]), len(xtrain_glove[i]))\n",
    "print (len_min, len_max)\n",
    "print (len(xtrain_glove[0][0]))\n",
    "print (len(xtrain_glove[1][-1]))\n",
    "print (type(xtrain_glove[0]))\n",
    "print (type(xtrain_glove[0][0]))\n",
    "\n",
    "xtrain_glove = np.array(xtrain_glove)\n",
    "print (xtrain_glove.shape)\n",
    "print (type(xtrain_glove[0, :]))\n",
    "print (type(xtrain_glove[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (ytrain[: 20])\n",
    "print (type(xtrain_glove), xtrain_glove.shape)\n",
    "xtrain_tmp = [[xtrain_glove[i], ytrain[i]] for i in range(len(xtrain_glove))]\n",
    "xtrain_tmp = np.array(sorted(xtrain_tmp, key = lambda x: len(x[0]), reverse = True))\n",
    "xtrain_sort_len = xtrain_tmp[: , 0]\n",
    "ytrain = xtrain_tmp[: , 1]\n",
    "print (type(ytrain))\n",
    "print (ytrain[: 20])\n",
    "for i in range(10):\n",
    "    print (i, len(xtrain_sort_len[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(xtrain_glove), xtrain_glove.shape)\n",
    "xtrain_torch = torch.from_numpy(xtrain_glove)\n",
    "print (xtrain_torch.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_input_x_packed = pack_padded_sequence(xtrain_glove, xtrain_len, batch_first=True)\n",
    "encoder_outputs_packed, (h_last, c_last) = self.lstm(embed_input_x_packed)\n",
    "encoder_outputs, _ = pad_packed_sequence(encoder_outputs_packed, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义模型\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space，相当于一个全连接层\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))  # 把三维张量转化为二级张量\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "### 模型训练及预测\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()    # 调用时形式为：预测值(N*C),label(N)。其中N为序列中word数，C为label的类别数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)  # 此处的inputs只能是一个sequence\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)    #一个sequence对应的词性标注list\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(\"[result]tag_scores={}\".format(tag_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
