{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import psutil\n",
    "import gc\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "# from sklearn.externals import joblib\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from glob import glob\n",
    "from sys import getsizeof\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "import shutil\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchtext import data, datasets\n",
    "from nltk import ngrams\n",
    "from torchtext.vocab import GloVe, Vectors\n",
    "from collections import defaultdict\n",
    "data_path = '/kaggle/input/spooky-author-identification/'\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "stop_words = stopwords.words('english')\n",
    "from torch.nn import utils as nn_utils\n",
    "print (torch.cuda.is_available())\n",
    "\n",
    "data_path = r'D:\\kaggle\\data\\spooky-author-identification\\a'[: -1]\n",
    "data_path_inv = r'D:\\kaggle\\data\\spooky-author-identification\\a'[: -1]\n",
    "data_path_word_vector = r'D:\\kaggle\\data\\word_vector\\a'[: -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 3) (8392, 2) (8392, 4)\n",
      "        id                                               text author\n",
      "0  id26305  This process, however, afforded me no means of...    EAP\n",
      "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
      "2  id11008  In his left hand was a gold snuff box, from wh...    EAP \n",
      "\n",
      "         id                                               text\n",
      "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
      "1  id24541  If a fire wanted fanning, it could readily be ...\n",
      "2  id00134  And when they had broken down the frail door t... \n",
      "\n",
      "         id       EAP       HPL       MWS\n",
      "0  id02310  0.403494  0.287808  0.308698\n",
      "1  id24541  0.403494  0.287808  0.308698\n",
      "2  id00134  0.403494  0.287808  0.308698\n"
     ]
    }
   ],
   "source": [
    "def read_data():\n",
    "    df_train = pd.read_csv(data_path_inv + r'train/train.csv')\n",
    "    df_test = pd.read_csv(data_path + 'test/test.csv')\n",
    "    df_sub = pd.read_csv(data_path + 'sample_submission/sample_submission.csv')\n",
    "    return df_train, df_test, df_sub\n",
    "df_train, df_test, df_sub = read_data()\n",
    "print (df_train.shape, df_test.shape, df_sub.shape)\n",
    "print (df_train.head(3), '\\n\\n', df_test.head(3), '\\n\\n', df_sub.head(3), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n",
      "(8392,)\n"
     ]
    }
   ],
   "source": [
    "# 处理数据\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(df_train.author.values)\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(df_train.text.values, y, \n",
    "                                                  stratify = y, \n",
    "                                                  random_state = 2020, \n",
    "                                                  test_size = 0.1, shuffle = True)\n",
    "xtest = df_test.text.values\n",
    "print (xtrain.shape)\n",
    "print (xvalid.shape)\n",
    "print (xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 500000, time:40.3858962059021\n",
      "index: 1000000, time:80.61405372619629\n",
      "index: 1500000, time:121.12958931922913\n",
      "index: 2000000, time:161.22476959228516\n",
      "Found 25931 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "embeddings_word_set = set()\n",
    "def sent2vec_add_word(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    for w in words:\n",
    "        embeddings_word_set.add(str.encode(w))\n",
    "    return 0\n",
    "\n",
    "temp = [sent2vec_add_word(x) for x in xtrain]\n",
    "del temp\n",
    "gc.collect()\n",
    "temp = [sent2vec_add_word(x) for x in xvalid]\n",
    "del temp\n",
    "gc.collect()\n",
    "temp = [sent2vec_add_word(x) for x in xtest]\n",
    "del temp\n",
    "gc.collect()\n",
    "\n",
    "# %% [code]\n",
    "# load the GloVe vectors in a dictionary:\n",
    "embeddings_index = {}\n",
    "f = open(data_path_word_vector + 'glove.840B.300d.txt', 'rb')\n",
    "index = 0\n",
    "pre_time = time.time()\n",
    "for line in f: # tqdm(f):\n",
    "    index += 1\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    if word in embeddings_word_set:\n",
    "        embeddings_index[word] = coefs\n",
    "    if index % 500000 == 0:\n",
    "        print ('index: {:}, time:{:}'.format(index, time.time() - pre_time))\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            torch_tmp = list(embeddings_index[str.encode(w)])\n",
    "            M.append(torch_tmp)\n",
    "        except:\n",
    "            continue\n",
    "    if len(M) == 0:\n",
    "        M.append([0] * 300)\n",
    "    return M\n",
    "\n",
    "xtrain_glove = [sent2vec(x) for x in xtrain]\n",
    "xvalid_glove = [sent2vec(x) for x in xvalid]\n",
    "xtest_glove = [sent2vec(x) for x in xtest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.12 GiB (GPU 0; 6.00 GiB total capacity; 4.16 GiB already allocated; 604.89 MiB free; 817.00 KiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-8762ef91b809>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxtrain_torch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mxtrain_torch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.12 GiB (GPU 0; 6.00 GiB total capacity; 4.16 GiB already allocated; 604.89 MiB free; 817.00 KiB cached)"
     ]
    }
   ],
   "source": [
    "xtrain_torch = torch.zeros((10000, 50, 300)).long().cuda()\n",
    "print (xtrain_torch.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_vector_size: 300, label_size: 3\n",
      "tensor(252, device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.94 GiB (GPU 0; 6.00 GiB total capacity; 3.94 GiB already allocated; 833.89 MiB free; 817.00 KiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-26e8ef922f32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mxtrain_lengths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mxtrain_torch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain_glove\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_vector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain_glove\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mseqlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain_lengths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.94 GiB (GPU 0; 6.00 GiB total capacity; 3.94 GiB already allocated; 833.89 MiB free; 817.00 KiB cached)"
     ]
    }
   ],
   "source": [
    "word_vector_size = len(xtrain_glove[0][0])\n",
    "label_size = 3\n",
    "print ('word_vector_size: {:}, label_size: {:}'.format(word_vector_size, label_size))\n",
    "\n",
    "xtrain_lengths = torch.LongTensor([len(x) for x in xtrain_glove]).cuda()\n",
    "print (xtrain_lengths.max())\n",
    "max_length = 100\n",
    "xtrain_torch = torch.zeros((len(xtrain_glove), max_length, word_vector_size)).long().cuda()\n",
    "for idx in range(len(xtrain_glove)):\n",
    "    seqlen = min(int(xtrain_lengths[idx].cpu().numpy()), max_length)\n",
    "    xtrain_torch[idx, :seqlen] = torch.LongTensor(np.array(xtrain_glove[idx])[: seqlen, :])\n",
    "\n",
    "print (type(xtrain_torch), xtrain_torch.size())\n",
    "xtrain_lengths, seq_idx = xtrain_lengths.sort(0, descending = False)\n",
    "xtrain_torch = xtrain_torch[seq_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(xtrain_glove), xtrain_glove.shape)\n",
    "len_min = 10000000\n",
    "len_max = 0\n",
    "for i in range(len(xtrain_glove)):\n",
    "    len_min = min(len_min, len(xtrain_glove[i]))\n",
    "    len_max = max(len_max, len(xtrain_glove[i]))\n",
    "    # print (type(xtrain_glove[i]), len(xtrain_glove[i]))\n",
    "print (len_min, len_max)\n",
    "print (len(xtrain_glove[0][0]))\n",
    "print (len(xtrain_glove[1][-1]))\n",
    "print (type(xtrain_glove[0]))\n",
    "print (type(xtrain_glove[0][0]))\n",
    "\n",
    "xtrain_glove = np.array(xtrain_glove)\n",
    "print (xtrain_glove.shape)\n",
    "print (type(xtrain_glove[0, :]))\n",
    "print (type(xtrain_glove[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (ytrain[: 20])\n",
    "print (type(xtrain_glove), xtrain_glove.shape)\n",
    "xtrain_tmp = [[xtrain_glove[i], ytrain[i]] for i in range(len(xtrain_glove))]\n",
    "xtrain_tmp = np.array(sorted(xtrain_tmp, key = lambda x: len(x[0]), reverse = True))\n",
    "xtrain_sort_len = xtrain_tmp[: , 0]\n",
    "ytrain = xtrain_tmp[: , 1]\n",
    "print (type(ytrain))\n",
    "print (ytrain[: 20])\n",
    "for i in range(10):\n",
    "    print (i, len(xtrain_sort_len[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(xtrain_glove), xtrain_glove.shape)\n",
    "xtrain_torch = torch.from_numpy(xtrain_glove)\n",
    "print (xtrain_torch.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_input_x_packed = pack_padded_sequence(xtrain_glove, xtrain_len, batch_first=True)\n",
    "encoder_outputs_packed, (h_last, c_last) = self.lstm(embed_input_x_packed)\n",
    "encoder_outputs, _ = pad_packed_sequence(encoder_outputs_packed, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义模型\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space，相当于一个全连接层\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))  # 把三维张量转化为二级张量\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "### 模型训练及预测\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()    # 调用时形式为：预测值(N*C),label(N)。其中N为序列中word数，C为label的类别数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)  # 此处的inputs只能是一个sequence\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)    #一个sequence对应的词性标注list\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(\"[result]tag_scores={}\".format(tag_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
